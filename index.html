<!DOCTYPE HTML>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-67ZL1LKCB1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-67ZL1LKCB1');
  </script>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shutong Zhang</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    /* Initially, the paragraph is hidden */
    .myParagraph {
      display: none;
    }

    .toggleButton {
      cursor: pointer;
    }
  </style>

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data/favicon.png">
</head>

<body>
  <table
    style="width:100%;max-width:1100px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:70%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Shutong (Tony) Zhang</name>
                  </p>
                  <p>
                    Hello! I am a first-year MSCS student from Stanford University, specializing in Artificial
                    Intelligence Track.
                    I received my Bachelor of Applied Science in Computer Engineering from University of Toronto with
                    the highest Honor.
                  </p>
                  <p>
                    In the past, I interned at the Vector Institute, working with <a
                      href="https://animesh.garg.tech/">Prof. Animesh Garg</a> and <a
                      href="https://www.cs.umd.edu/~lin/">Prof. Ming Lin</a> on computer vision and robotics.
                    I also spent a summer working with <a
                      href="https://vision.ee.ethz.ch/people-details.OTAyMzM=.TGlzdC8zMjg3LC0xOTcxNDY1MTc4.html">Prof.
                      Luc Van Gool</a> and <a href="https://people.ee.ethz.ch/~csakarid/">Dr. Christos Sakaridis</a> at
                    the ETH Zurich Computer Vision Lab, focusing on semantic scene understanding.
                    In addition, I worked with <a href="https://www.eecg.utoronto.ca/~shuruiz/">Prof. Shurui Zhou</a>
                    and <a href="https://jhcheng.me/">Prof. Jinghui Cheng</a> on collaboration beyond individuals.
                  </p>
                  <p style="text-align:center">
                    <!-- <a href="data/CV.pdf">CV</a> &nbsp/&nbsp -->
                    <a href="mailto:shutong.zhang@mail.utoronto.ca">Email</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/shutong-tony-zhang/">Linkedin</a> &nbsp/&nbsp
                    <!-- <a href="assets/CV_Shutong_Zhang.pdf">CV</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=JYMjWq8AAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="../misc/index.html">Misc</a>
                  </p>
                  <!-- <p style="text-align:center">
                    Navigate to:
                    <a href="#education"> Education</a> /
                    <a href="#publication-and-papers"> Publications</a> /
                    <a href="#research-experience"> Research Experience</a> /
                    <a href="#work-experience"> Work Experience</a> /
                    <a href="#teaching"> Teaching & Outreach</a> /
                    <a href="#awards-and-honors"> Awards</a>
                  </p> -->
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <img style="width:85%;max-width:100%" alt="profile photo" src="images/image.png"
                    class="hoverZoomLink">
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading id="education">News</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tr>
              <p style="padding-left: 20px;">[2025.6] I started my internship at Google!</p>
              <p style="padding-left: 20px;">[2025.5] I am Honored to receive the 'Well-Done' Course Assistant bonus from Stanford CS Department!</p>
              <p style="padding-left: 20px;">[2024.11] My paper SOLO got accepted by WACV 2025 as Oral Presentation!</p>
              <p style="padding-left: 20px;">[2024.9] My first-author paper on SE&UX Collaboration got accepted by CSCW
                2025!</p>
              <p style="padding-left: 20px;">[2024.6] I graduated from University of Toronto with the highest honor!</p>
              <p style="padding-left: 20px;">[2024.4] I will join Stanford University as a Master's student in Computer
                Science!</p>
              <p style="padding-left: 20px;">[2024.1] My first-author paper HandyPriors got accepted by ICRA 2024!</p>
            </tr>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tbody>
                <tr>
                  <td>
                    <heading id="education">Education</heading>
                  </td>
                </tr>
              </tbody>
            </table>
            <table width="100%" align="center" border="0" cellpadding="20">
              <tbody>

                <tr>
                  <td align="center" style="padding:20px;width:30%;vertical-align:middle">
                    <img src="images/stanford.webp" width="200">
                  </td>
                  <td width="75%" valign="middle">
                    Stanford University, CA, United States <br>
                    Master of Science in Computer Science <br>
                    2024 - 2026<br>
                    Specialized in Artificial Intelligence<br>
                    CGPA: 4.11/4.00
                  </td>
                </tr>


                <tr>
                  <td align="center" style="padding:20px;width:30%;vertical-align:middle">
                    <img src="images/uoft.png" width="200">
                  </td>
                  <td width="75%" valign="middle">
                    University of Toronto, ON, Canada <br>
                    Bachelor of Applied Science <br>
                    2019 - 2024<br>
                    Major in Computer Engineering, minor in Artificial Intelligence <br>
                    CGPA: 3.94/4.00
                  </td>
                </tr>




                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                  <tbody>
                    <tr>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading id="publication-and-papers">Publications and Papers</heading>
                      </td>
                    </tr>
                  </tbody>
                </table>
                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                  <tbody>

                    <tr>
                      <td style="padding:20px;width:40%;display: flex;align-items: center;">
                        <img src='images/version_compare.png' width="300">
                      </td>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <papertitle>Untangling the Timeline: Challenges and Opportunities in Supporting Version Control
                          in Modern Computer-Aided Design</papertitle>
                        <br>
                        <a href="https://felix-deng.github.io/">Yuanzhe (Felix) Deng</a>,
                        <strong>Shutong Zhang</strong>,
                        <a href="https://cheng-kathy.github.io/home/">Kathy Cheng</a>,
                        <a href="https://scholar.google.ca/citations?user=P5yUQ-IAAAAJ&hl=en">Alison Olechowski</a>,
                        <a href="https://www.eecg.utoronto.ca/~shuruiz/">Shurui Zhou</a>
                        <br>
                        Under review
                        <br>
                        <a class="toggleButton">abstract</a>
                        /
                        paper (coming soon...)

                        <p class="myParagraph">Version control has been implemented in computer-aided design (CAD) to
                          support mechanical product design
                          by enabling traceability, managing product variation, and supporting collaboration. However,
                          there currently lacks a systematic
                          review of the challenges of performing version control in modern CAD software from a user
                          perspective. In this paper, we mined and
                          analyzed 424 posts and corresponding discussions from online CAD user forums to identify the
                          challenges that CAD users face when
                          using version control for CAD modelling and sharing, concerning the management, continuity,
                          scope, and distribution of versions.
                          We further evaluated the functionalities across three commercially available CAD software with
                          built-in version control capabilities
                          and identified four key challenges that universally exist across all three software. For each
                          key challenge, we discussed corresponding
                          design implications for improving version control in CAD with reference to the
                          state-of-the-art version control solution in software
                          development. This paper concludes with implications for CAD software providers and the CSCW
                          community regarding the future research and
                          development of version control in CAD. </p>
                        <p></p>
                      </td>
                    </tr>

                    <tr>
                      <td style="padding:20px;width:40%;display: flex;align-items: center;">
                        <img src='images/solo.png' width="300">
                      </td>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <papertitle>Sun Off, Lights On: Photorealistic Monocular Nighttime Simulation for Robust
                          Semantic Perception</papertitle>
                        <br>
                        <a
                          href="https://icu.ee.ethz.ch/people/person-detail.MzIxMjU4.TGlzdC8zMjQxLC0xMjMyNzYwMzQy.html">Konstantinos
                          Tzevelekakis</a>,
                        <strong>Shutong Zhang</strong>,
                        <a
                          href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Luc
                          Van Gool</a>,
                        <a href="https://people.ee.ethz.ch/~csakarid/">Christos Sakaridis</a>
                        <br>
                        Accepted by the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025 - <b
                          style="color: red;">Oral Presentation</b>
                        <br>
                        <a class="toggleButton">abstract</a>
                        /
                        <a href="http://arxiv.org/abs/2407.20336">paper</a>
                        /
                        <a href="https://ktzevel.github.io/SOLO/">project page</a>
                        <p class="myParagraph">Nighttime scenes are hard to semantically perceive with
                          learned models and annotate for humans. Thus, realistic synthetic nighttime
                          data become all the more important for learning robust semantic perception at
                          night, thanks to their accurate and cheap semantic annotations. However,
                          existing data-driven or hand-crafted techniques for generating nighttime images
                          from daytime counterparts suffer from poor realism. The reason is the complex
                          interaction of highly spatially varying nighttime illumination, which differs
                          drastically from its daytime counterpart, with objects of spatially varying
                          materials in the scene, happening in 3D and being very hard to capture with
                          such 2D approaches. The above 3D interaction and illumination shift have
                          proven equally hard to \emph{model} in the literature, as opposed to other
                          conditions such as fog or rain. Our method, named Sun Off, Lights On (SOLO),
                          is the first to perform nighttime simulation on single images in a photorealistic
                          fashion by operating in 3D. It first explicitly estimates the 3D geometry, the
                          materials and the locations of light sources of the scene from the input daytime
                          image and relights the scene by probabilistically instantiating light sources in
                          a way that accounts for their semantics and then running standard ray tracing.
                          Not only is the visual quality and photorealism of our nighttime images superior
                          to competing approaches including diffusion models, but the former images are also
                          proven more beneficial for semantic nighttime segmentation in day-to-night adaptation.</p>
                        <p></p>
                      </td>
                    </tr>

                    <tr>
                      <td style="padding:20px;width:40%;display: flex;align-items: center;">
                        <img src='images/fse_2024.png' width="300">
                      </td>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <papertitle>Who is to Blame: A Comprehensive Review of Challenges and
                          Opportunities in Designer-Developer Collaboration</papertitle>
                        <br>
                        <strong>Shutong Zhang</strong>,
                        <a href="https://www.linkedin.com/in/tianyu-zhang-869b621bb/?originalSubdomain=ca">Tianyu
                          Zhang</a>,
                        <a href="https://jhcheng.me/">Jinghui Cheng</a>,
                        <a href="https://www.eecg.utoronto.ca/~shuruiz/">Shurui Zhou</a>
                        <br>
                        Accepted by the ACM Conference on Computer-Supported Cooperative Work & Social Computing (CSCW),
                        2025
                        <br>
                        <a class="toggleButton">abstract</a>
                        /
                        <a href="https://arxiv.org/abs/2501.11748">paper</a> /
                        <a href="https://github.com/FORCOLAB-UofToronto/CSCW2025-WhoIsToBlame">project page</a>
                        <p class="myParagraph">Software development relies on effective collaboration between Software
                          Development Engineers (SDEs)
                          and User eXperience Designers (UXDs) to create software products of high quality and
                          usability.
                          While this
                          collaboration issue has been explored over the past decades, anecdotal evidence continues to
                          indicate the
                          existence of challenges in their collaborative efforts. To understand this gap, we first
                          conducted a systematic
                          literature review of 44 papers published since 2005, uncovering three key collaboration
                          challenges and two main
                          best practices. We then analyzed designer and developer forums and discussions on open-source
                          software
                          repositories to assess how the challenges and practices manifest in the status quo. Our
                          findings
                          have broad
                          applicability for collaboration in software development, extending beyond the partnership
                          between SDEs and
                          UXDs. The suggested best practices and interventions also act as a reference for future
                          research, assisting in
                          the development of dedicated collaboration tools for SDEs and UXDs.</p>
                        <p></p>
                      </td>
                    </tr>

                    <tr>
                      <td style="padding:20px;width:40%;display: flex;align-items: center;">
                        <img src='images/eth.png' width="300">
                      </td>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <papertitle>NPSim: Nighttime Photorealistic Simulation From Daytime Images With Monocular
                          Inverse Rendering and Ray Tracing</papertitle>
                        <br>
                        <strong>Shutong Zhang</strong>
                        <br>
                        Thesis at ETH Zurich Computer Vision Lab
                        <br>
                        <a class="toggleButton">abstract</a>
                        /
                        <a href="data/Thesis.pdf">paper</a>
                        /
                        <a href="data/Slides.pdf">slides</a> <br>
                        <p class="myParagraph">Semantic segmentation is an important task for autonomous driving. A
                          powerful autonomous driving system
                          should be capable of handling images under all conditions, including nighttime. Generating
                          accurate and diverse nighttime
                          semantic segmentation datasets is crucial for enhancing the performance of computer vision
                          algorithms in low-light conditions.
                          In this thesis, we introduce a novel approach named NPSim, which enables the simulation of
                          realistic nighttime images from real
                          daytime counterparts with monocular inverse rendering and ray tracing. NPSim comprises two key
                          components: mesh reconstruction
                          and relighting. The mesh reconstruction component generates an accurate representation of the
                          scene’s structure by combining
                          geometric information extracted from the input RGB image and semantic information from its
                          corresponding semantic labels. The
                          relighting component integrates real-world nighttime light sources and material
                          characteristics
                          to simulate the complex interplay
                          of light and object surfaces under low-light conditions. The scope of this thesis mainly
                          focuses
                          on the implementation and
                          evaluation of the mesh reconstruction component. Through experiments, we demonstrate the
                          effectiveness of the mesh reconstruction
                          component in producing high-quality scene meshes and their generality across different
                          autonomous driving datasets. We also
                          propose a detailed experiment plan for evaluating the entire pipeline, including both
                          quantitative metrics in training
                          state-of-the-art supervised and unsupervised semantic segmentation approaches and human
                          perceptual studies, aiming to indicate the
                          capability of our approach to generate realistic nighttime images and the value of our dataset
                          in steering future progress in the
                          field. NPSim not only has the ability to address the scarcity of nighttime datasets for
                          semantic
                          segmentation, but it also has the
                          potential to improve the robustness and performance of vision algorithms under low-lighting
                          conditions.</p>
                        <p></p>
                      </td>
                    </tr>

                    <tr>
                      <td style="padding:20px;width:40%;display: flex;align-items: center;">
                        <img src='images/icra_2024.jpg' width="300">
                      </td>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <papertitle>HandyPriors: Physically Consistent Perception of Hand-Object Interactions with
                          Differentiable Priors</papertitle>
                        <br>
                        <strong>Shutong Zhang</strong>*,
                        <a href="https://ylqiao.net/">Yiling Qiao*</a>,
                        <a href="https://www.linkedin.com/in/guangleizhu0818/">Guanglei Zhu*</a>,
                        <a href="https://eric-heiden.com/">Eric Heiden</a>,
                        <a href="http://www.cs.toronto.edu/~dylanturpin/">Dylan Turpin</a>,
                        <a href="#">Jingzhou Liu</a>,
                        <a href="https://www.cs.umd.edu/~lin/">Ming Lin</a>,
                        <a href="http://blog.mmacklin.com/">Miles Macklin</a>, <br>
                        <a href="https://animesh.garg.tech/">Animesh Garg</a>
                        <br>
                        Accepted by Conference on Computer Vision and Pattern Recognition Workshop <em>(CVPRW)</em>,
                        2023<br>
                        Accepted by IEEE International Conference on Robotics and Automation <em>(ICRA)</em>, 2024
                        <br>
                        <a class="toggleButton">abstract</a> /
                        <a href="https://arxiv.org/abs/2311.16552">paper</a> /
                        <a href="https://handypriors.github.io/">project page</a>
                        <br>
                        <p class="myParagraph"> Various heuristic objectives for modeling hand-
                          object interaction have been proposed in past work. However,
                          due to the lack of a cohesive framework, these objectives
                          often possess a narrow scope of applicability and are limited
                          by their efficiency or accuracy. In this paper, we propose
                          HANDYPRIORS, a unified and general pipeline for human-
                          object interaction scenes by leveraging recent advances in
                          differentiable physics and rendering. Our approach employs
                          rendering priors to align with input images and segmenta-
                          tion masks along with physics priors to mitigate penetration
                          and relative-sliding across frames. Furthermore, we present
                          two alternatives for hand and object pose estimation. The
                          optimization-based pose estimation achieves higher accuracy,
                          while the filtering-based tracking, which utilizes the differen-
                          tiable priors as dynamics and observation models, executes
                          faster. We demonstrate that HANDYPRIORS attains comparable
                          or superior results in the pose estimation task, and that the
                          differentiable physics module can predict contact information
                          for pose refinement. We also show that our approach generalizes
                          to perception tasks, including robotic hand manipulation and
                          human-object pose estimation in the wild.</p>
                        <p></p>
                      </td>
                    </tr>

                    <tr>
                      <td style="padding:20px;width:40%;display: flex;align-items: center;">
                        <img src='images/icra_2023.png' width="300">
                      </td>
                      <td style="padding:20px;width:100%;vertical-align:middle">
                        <papertitle>Fast-Grasp’D: Dexterous Multi-finger Grasp Generation Through Differentiable
                          Simulations</papertitle>
                        <br>
                        <a href="http://www.cs.toronto.edu/~dylanturpin/">Dylan Turpin</a>,
                        <a href="https://www.taozhong.info/">Tao Zhong</a>,
                        <strong>Shutong Zhang</strong>,
                        <a href="https://www.linkedin.com/in/guangleizhu0818/">Guanglei Zhu</a>,
                        <a href="https://eric-heiden.com/">Eric Heiden</a>,
                        <a href="http://blog.mmacklin.com/">Miles Macklin</a>,
                        <a href="https://tsogkas.github.io/">Stavros Tsogkas</a>,
                        <a href="https://www.cs.toronto.edu/~sven/">Sven Dickinson</a>,
                        <a href="https://animesh.garg.tech/">Animesh Garg</a>
                        <br>
                        Accepted by IEEE International Conference on Robotics and Automation <em>(ICRA)</em>, 2023
                        <br>
                        <a class="toggleButton">abstract</a>
                        /
                        <a href="https://ieeexplore.ieee.org/document/10160314">paper</a>
                        /
                        <a href="https://dexgrasp.github.io/">project page</a> <br>
                        <p class="myParagraph"> Multi-finger grasping relies on high quality training
                          data, which is hard to obtain: human data is hard to transfer
                          and synthetic data relies on simplifying assumptions that reduce
                          grasp quality. By making grasp simulation differentiable, and
                          contact dynamics amenable to gradient-based optimization,
                          we accelerate the search for high-quality grasps with fewer
                          limiting assumptions. We present Grasp’D-1M: a large-scale
                          dataset for multi-finger robotic grasping, synthesized with Fast-
                          Grasp’D, a novel differentiable grasping simulator. Grasp’D-
                          1M contains one million training examples for three robotic
                          hands (three, four and five-fingered), each with multimodal
                          visual inputs (RGB+depth+segmentation, available in mono and
                          stereo). Grasp synthesis with Fast-Grasp’D is 10x faster than
                          GraspIt! and 20x faster than the prior Grasp’D differentiable
                          simulator. Generated grasps are more stable and contact-rich
                          than GraspIt! grasps, regardless of the distance threshold used
                          for contact generation. We validate the usefulness of our dataset
                          by retraining an existing vision-based grasping pipeline
                          on Grasp’D-1M, and showing a dramatic increase in model
                          performance, predicting grasps with 30% more contact, a 33%
                          higher epsilon metric, and 35% lower simulated displacement. </p>
                        <p></p>
                      </td>
                    </tr>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                      <tbody>
                        <tr>
                          <td>
                            <heading id="work-experience">Work Experience</heading>
                          </td>
                        </tr>
                      </tbody>
                    </table>
                    <table width="100%" align="center" border="0" cellpadding="20">
                      <tbody>

                        <tr>
                          <td style="padding:40px;width:40%;display: flex;align-items: center;">
                            <img src='images/google_logo.png' width="250">
                          </td>
                          <td width="75%" valign="center">
                            <b> Google LLC </b>, United States <br>
                            2025.6 - Present <br>
                            <br>
                            <b>Software Engineer (Machine Learning) Intern</b> <br>
                            Youtube GenAI for Content Safety <br>
                          </td>
                        </tr>

                        <tr>
                          <td style="padding:20px;width:40%;display: flex;align-items: center;">
                            <img src='images/intel_logo.png' width="300">
                          </td>
                          <td width="75%" valign="center">
                            <b> Intel Corporation </b>, Canada <br>
                            2022.5 - 2023.4 <br>
                            <br>
                            <b>Software Engineer</b> <br>
                            Quality and Execution Team: Project Manager and Software Engineer <br>
                            Customer Happiness and User Experience Team: Front-End Developer <br>
                            Core Datapath Team: Compiler Engineer <br>
                          </td>
                        </tr>

                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                          <tbody>
                            <tr>
                              <td>
                                <heading id="research-experience">Research Experience</heading>
                              </td>
                            </tr>
                          </tbody>
                        </table>
                        <table width="100%" align="center" border="0" cellpadding="20">
                          <tbody>

                            <tr>
                              <td style="padding:20px;width:40%;display: flex;align-items: center;">
                                <img src='images/pair_vector.jpg' width="300">
                              </td>
                              <td width="75%" valign="center">
                                <b> PAIR Lab and Vector Institute </b>, Canada <br>
                                2023.8 - 2024.5 <br>
                                <br>
                                <b>Machine Learning Research Intern</b> <br>
                                Supervisor: <a href="https://animesh.garg.tech/" target="_blank">Prof. Animesh Garg</a>,
                                with
                                <a href="https://www.cs.umd.edu/~lin/" target="_blank">Prof. Ming C. Lin</a>
                              </td>
                            </tr>

                            <tr>
                              <td style="padding:20px;width:40%;display: flex;align-items: center;">
                                <img src='images/cvl.png' width="300">
                              </td>
                              <td width="75%" valign="center">
                                <b> ETH Zurich Computer Vision Lab </b>, Switzerland <br>
                                2023.4 - 2023.8 <br>
                                <br>
                                <b>Computer Vision Research Intern</b> <br>
                                Supervisor: <a
                                  href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Prof.
                                  Luc Van Gool</a> and <a href="https://people.ee.ethz.ch/~csakarid/">Dr. Christos
                                  Sakaridis</a>
                              </td>
                            </tr>

                            <tr>
                              <td style="padding:20px;width:40%;display: flex;align-items: center;">
                                <img src='images/forcolab.png' width="300">
                              </td>
                              <td width="75%" valign="center">
                                <b> Forcolab </b>, Canada <br>
                                2022.4 - 2023.9 <br>
                                <br>
                                <b>Research Fellow</b> <br>
                                Supervisor: <a href="https://www.eecg.utoronto.ca/~shuruiz/" target="_blank">Prof.
                                  Shurui
                                  Zhou</a>, with <a href="https://jhcheng.me/" target="_blank">Prof. Jinghui Cheng</a>
                              </td>
                            </tr>

                            <table
                              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                              <tbody>
                                <tr>
                                  <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading id="publication-and-papers">Selected Projects</heading>
                                  </td>
                                </tr>
                              </tbody>
                            </table>
                            <table
                              style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                              <tbody>

                                <tr>
                                  <td style="padding:45px;width:40%;display: flex;align-items: right;">
                                    <img src='images/watch_and_listen.png' width="250">
                                  </td>
                                  <td style="padding:20px;width:100%;vertical-align:middle">
                                    <papertitle>Watch and Listen: Personalized Video Summarization with Multi-Modal Learning</papertitle> <br>
                                    Stanford CS224n Natural Language Processing with Deep Learning Final Project<br>
                                    <a class="toggleButton">abstract</a>
                                    <p class="myParagraph">Video summarization is key to managing the ever-growing volume of video content. Personalized 
                                      summarization, which tailors highlights to individual preferences, offers a more engaging and relevant viewing experience. 
                                      However, understanding user preferences from video content requires leveraging multiple modalities, including visual, 
                                      auditory, and textual information. Traditional approaches often fail to integrate these modalities effectively, limiting 
                                      their ability to create accurate and personalized summaries. In this project, we introduce a multi-modal learning framework
                                       that combines information from both video and audio sources to create personalized video summaries. Evaluation results on 
                                       benchmark datasets demonstrate that our approach achieves competitive performance relative to state-of-the-art models. 
                                       Overall, our framework advances video summarization by incorporating diverse modalities to produce contextually rich and 
                                       personalized summaries, offering promising directions for future research in dynamic multimedia environments. </p>
                                    <p></p>
                                  </td>
                                </tr>
                                
                                <br>

                                <tr>
                                  <td style="padding:20px;width:40%;display: flex;align-items: center;">
                                    <img src='images/llm_rl.png' width="300">
                                  </td>
                                  <td style="padding:20px;width:100%;vertical-align:middle">
                                    <papertitle>Fine-Tuning Large Language Models with Reinforcement Learning</papertitle>
                                    <br>
                                    Stanford CS224r Deep Reinforcement Learning Final Project<br>
                                    <a class="toggleButton">abstract</a>
                                    <p class="myParagraph">Large language models (LLMs) have demonstrated strong performance across a range of tasks, including 
                                      text generation, image classification, and reasoning. To better align these base models with specific user needs, 
                                      fine-tuning plays a crucial role. In this project, we aim to improve a large language model’s instruction following and 
                                      math reasoning abilities through fine-tuning. Specifically, we work on Qwen-2.5-0.5B model, apply Supervised Fine-Tuning 
                                      (SFT) and incorporate reinforcement learning techniques, including Direct Preference Optimization (DPO) and REINFORCE Leave-One-Out 
                                      (RLOO). For the instruction following task, we use the SmolTalk dataset and the UltraFeedback dataset for SFT and DPO, respectively. 
                                      We also incorporate Reinforcement Learning from AI Feedback (RLAIF) for preference scoring. For the math reasoning task, we use the
                                       WarmStart dataset and the Countdown dataset for SFT and RLOO, respectively. To further boost the accuracy, we use a calculator tool 
                                       during both training and inference time.</p>
                                    <p></p>
                                  </td>
                                </tr>

      </tr>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <heading id="teaching">Teaching and Outreach</heading>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>
          <p style="padding-left: 20px;"><b><a href="https://cs231n.stanford.edu/">Stanford University CS231n Deep
                Learning for Computer Vision</a></b>, Course Assistant. 2025 Spring</p>
          <p style="padding-left: 20px;"><b><a
                href="https://engineering.calendar.utoronto.ca/course/ece243h1">University of
                Toronto ECE243 Computer Organization</a></b>, Teaching Assistant. 2022 Winter, 2023
            Winter</p>
          <p style="padding-left: 20px;"><b><a
                href="https://engineering.calendar.utoronto.ca/course/ece253h1">University of
                Toronto ECE253 Digital and Computer Systems</a></b>, Teaching Assistant. 2021 Fall,
            2022 Fall</p>
          <p style="padding-left: 20px;"><b><a
                href="https://undergrad.engineering.utoronto.ca/skule-life/engineering-society/">University
                of Toronto Engineering</a></b>, Mentor. 2020 - present</p>
          <p style="padding-left: 20px;"><b>Rural Teaching Volunteer Program</b>, Physics Teacher.
            2020</p>
          <p style="padding-left: 20px;"><b>EngFastlane</b>, Calculas and Mechanics Instructor. 2019
            - 2020</p>

        </tbody>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody>
          <tr>
            <td>
              <heading id="awards-and-honors">Awards and Honors</heading>
            </td>
          </tr>
        </tbody>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">
        <tbody>

          <p style="padding-left: 20px;">Excellent Cource Assistant Award ($1400). June 2025</p>
          <p style="padding-left: 20px;">International Experience Award ($3000). May 2023</p>
          <p style="padding-left: 20px;">University of Toronto Summer Research Exchange Fellowship
            ($3000). Dec 2022</p>
          <p style="padding-left: 20px;">Edith Grace Buchan Undergraduate Research Fellowship
            ($5400). Apr 2022</p>
          <p style="padding-left: 20px;">Department of Electrical and Computer Engineering Top
            Student Award. Oct 2021</p>
          <p style="padding-left: 20px;">University of Toronto In Course Scholarship ($1500). Aug
            2021</p>
          <p style="padding-left: 20px;">University of Toronto Scholar. Aug 2021</p>
          <p style="padding-left: 20px;">University of Toronto Summer Research Fellowship ($5000).
            May 2021</p>
          <p style="padding-left: 20px;">Deans Honor List. 2019 -- 2022</p>
          <p style="padding-left: 20px;">Faculty Of Applied Science & Engineering Admission
            Scholarship ($5000). Sep 2019</p>




        </tbody>
      </table>
      <table
        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template borrowed from <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
              </p>
            </td>
          </tr>
        </tbody>
      </table>
      </td>

  </table>
</body>

</html>

<script>
  // JavaScript code to toggle the paragraph's visibility
  // JavaScript code to toggle the paragraphs' visibility
  document.addEventListener("DOMContentLoaded", function () {
    const toggleButtons = document.querySelectorAll(".toggleButton");

    toggleButtons.forEach((button, index) => {
      button.addEventListener("click", function () {
        const correspondingParagraph = document.querySelectorAll(".myParagraph")[index];

        // Check the current display style of the paragraph
        if (correspondingParagraph.style.display === "none" || correspondingParagraph.style.display === "") {
          // Show the paragraph
          correspondingParagraph.style.display = "block";
        } else {
          // Hide the paragraph
          correspondingParagraph.style.display = "none";
        }
      });
    });
  });
</script>