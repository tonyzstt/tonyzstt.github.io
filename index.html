<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shutong Zhang</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    /* Initially, the paragraph is hidden */
    .myParagraph {
      display: none;
    }

    .toggleButton {
      cursor: pointer;
    }
  </style>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data/favicon.png">
</head>

<body>
  <table style="width:100%;max-width:1100px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shutong (Tony) Zhang</name>
              </p>
              <p>
              Welcome! I am a final-year Computer Engineering student from the <a href="https://www.utoronto.ca/" target="_blank">University of Toronto</a>.
              I am currently working with <a href="https://animesh.garg.tech/" target="_blank">Prof. Animesh Garg</a> at <a href="https://www.pair.toronto.edu/" target="_blank">PAIR Lab</a> 
              and <a href="https://vectorinstitute.ai/" target="_blank">Vector Insititute</a>. Previously, I am fortunate to work with 
              <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html" target="_blank">Prof. Luc Van Gool</a> 
              and <a href="https://people.ee.ethz.ch/~csakarid/" target="_blank">Dr. Christos Sakaridis</a> at <a href="https://vision.ee.ethz.ch/" target="_blank">Computer Vision Lab</a>, 
              <a href="https://ethz.ch/en.html" target="_blank">ETH Zurich</a>, <a href="https://www.eecg.utoronto.ca/~shuruiz/" target="_blank">Prof. Shurui Zhou</a> 
              and <a href="https://jhcheng.me/" target="_blank">Prof. Jinghui Cheng</a> at <a href="https://shuiblue.github.io/forcolab-uoft/" target="_blank">Forcolab</a>, 
              <a href="https://www.cs.umd.edu/~lin/" target="_blank">Prof. Ming C. Lin</a> at <a href="https://umd.edu/" target="_blank">University of Maryland</a> and 
              <a href="https://www.ece.utoronto.ca/people/chow-p/" target="_blank">Prof. Paul Chow</a> at <a href="https://www.eecg.utoronto.ca/" target="_blank">Computer Engineering Research Group</a>.
              </p>
              <p>
                I'm interested in Computer Vision, Robotics, and Software Engineering. On the Vision and Robotics side, 
                I wish to integrate <b>visual perceptions</b> into <b>physically plausible actions</b> to bridge the gap between perception 
                and planning, and ​​enable robots to perform complex tasks with strong generalization in the 3D world. On the Software 
                Engineering side, I wish to contribute to <b>collaboration beyond individuals</b>. My goal is to develop collaboration
                tools and criteria to <b>facilitate multidisciplinary collaboration</b>. For example, UX designers & Software Developers,
                Machine Learning Engineers & Software Engineers, and LLMs & Humans.
              </p>
              <p style="text-align:center">
                <a href="data/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="mailto:shutong.zhang@mail.utoronto.ca">Email</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/shutong-zhang-7b744a204">Linkedin</a> &nbsp/&nbsp
                <a href="https://github.com/Tonyzhang2000">Github</a> &nbsp/&nbsp
                <a href="../misc/index.html">Misc</a> 
              </p>
              <p style="text-align:center">
                Navigate to: 
                <a href="#education"> Education</a> / 
                <a href="#publication-and-papers"> Publications</a> /
                <a href="#research-experience"> Research Experience</a> /
                <a href="#work-experience"> Work Experience</a> /
                <a href="#teaching"> Teaching & Outreach</a> /
                <a href="#awards-and-honors"> Awards</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile_image.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_image.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading id="education">Education</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>


          <tr>
            <td align="center" style="padding:20px;width:30%;vertical-align:middle">
							<img src="images/uoft.png" width="200">
            </td>
            <td width="75%" valign="middle">
              University of Toronto, ON, Canada <br>
              Bachelor of Applied Science <br>
              2019.9-2024.6 (expected) <br>
              Major in Computer Engineering, minor in Artificial Intelligence <br>
              CGPA: 3.99/4.0, Major GPA: 4.0/4.0
            </td>
          </tr>

          


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading id="publication-and-papers">Publications and Papers</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    
          <tr>
            <td style="padding:20px;width:40%;display: flex;align-items: center;">
              <img src='images/fse_2024.png' width="300">
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="#">
                <papertitle>Who to Blame: A Comprehensive Review of Challenges and Opportunities in Designer-Developer Collaboration</papertitle>
              </a>
              <br>
              <strong>Shutong Zhang</strong>,
              <a href="https://www.linkedin.com/in/tianyu-zhang-869b621bb/?originalSubdomain=ca">Tianyu Zhang</a>,
              <a href="https://jhcheng.me/">Jinghui Cheng</a>,
              <a href="https://www.eecg.utoronto.ca/~shuruiz/">Shurui Zhou</a>
              <br>
              Submitted to ACM International Conference on the Foundations of Software Engineering <em>(FSE)</em>, 2024
              <br>
              <a class="toggleButton">abstract</a>
              /
              paper (coming soon...)
              <p class="myParagraph">Software development relies on effective collaboration between Software Development Engineers (SDEs)
                and User eXperience Designers (UXDs) to create software products of high quality and usability. While this
                collaboration issue has been explored over the past decades, anecdotal evidence continues to indicate the
                existence of challenges in their collaborative efforts. To understand this gap, we first conducted a systematic
                literature review of 44 papers published since 2005, uncovering three key collaboration challenges and two main
                best practices. We then analyzed designer and developer forums and discussions on open-source software
                repositories to assess how the challenges and practices manifest in the status quo. Our findings have broad
                applicability for collaboration in software development, extending beyond the partnership between SDEs and
                UXDs. The suggested best practices and interventions also act as a reference for future research, assisting in
                the development of dedicated collaboration tools for SDEs and UXDs.</p>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;display: flex;align-items: center;">
              <img src='images/eth.png' width="300">
            </td>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="#">
                <papertitle>NPSim: Nighttime Photorealistic Simulation From Daytime Images With Monocular Inverse Rendering and Ray Tracing</papertitle>
              </a>
              <br>
              <strong>Shutong Zhang</strong>
              <br>
              Thesis at ETH Zurich Computer Vision Lab (Aiming ECCV 2024)
              <br>
              <a class="toggleButton">abstract</a>
              /
              <a href="data/Thesis.pdf">paper</a>
              /
              <a href="data/Slides.pdf">slides</a> <br>
              <p class="myParagraph">Semantic segmentation is an important task for autonomous driving. A powerful autonomous driving system 
                should be capable of handling images under all conditions, including nighttime. Generating accurate and diverse nighttime 
                semantic segmentation datasets is crucial for enhancing the performance of computer vision algorithms in low-light conditions. 
                In this thesis, we introduce a novel approach named NPSim, which enables the simulation of realistic nighttime images from real 
                daytime counterparts with monocular inverse rendering and ray tracing. NPSim comprises two key components: mesh reconstruction 
                and relighting. The mesh reconstruction component generates an accurate representation of the scene’s structure by combining 
                geometric information extracted from the input RGB image and semantic information from its corresponding semantic labels. The 
                relighting component integrates real-world nighttime light sources and material characteristics to simulate the complex interplay 
                of light and object surfaces under low-light conditions. The scope of this thesis mainly focuses on the implementation and 
                evaluation of the mesh reconstruction component. Through experiments, we demonstrate the effectiveness of the mesh reconstruction 
                component in producing high-quality scene meshes and their generality across different autonomous driving datasets. We also 
                propose a detailed experiment plan for evaluating the entire pipeline, including both quantitative metrics in training 
                state-of-the-art supervised and unsupervised semantic segmentation approaches and human perceptual studies, aiming to indicate the 
                capability of our approach to generate realistic nighttime images and the value of our dataset in steering future progress in the 
                field. NPSim not only has the ability to address the scarcity of nighttime datasets for semantic segmentation, but it also has the 
                potential to improve the robustness and performance of vision algorithms under low-lighting conditions.</p>
              <p></p>
            </td>
          </tr>

    <tr>
      <td style="padding:20px;width:40%;display: flex;align-items: center;">
        <img src='images/icra_2024.jpg' width="300">
      </td>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <a href="#">
          <papertitle>HandyPriors: Physically Consistent Perception of Hand-Object Interactions with Differentiable Priors</papertitle>
        </a>
        <br>
        <strong>Shutong Zhang</strong>*,
        <a href="https://ylqiao.net/">Yiling Qiao*</a>,
        <a href="https://www.linkedin.com/in/guangleizhu0818/">Guanglei Zhu*</a>,
        <a href="https://eric-heiden.com/">Eric Heiden</a>, 
        <a href="http://www.cs.toronto.edu/~dylanturpin/">Dylan Turpin</a>,
        <a href="#">Jingzhou Liu</a>,
        <a href="https://www.cs.umd.edu/~lin/">Ming Lin</a>,
        <a href="http://blog.mmacklin.com/">Miles Macklin</a>, <br>
        <a href="https://animesh.garg.tech/">Animesh Garg</a>
        <br>
        Accepted by Conference on Computer Vision and Pattern Recognition Workshop <em>(CVPRW)</em>, 2023<br>
        Submitted to IEEE International Conference on Robotics and Automation <em>(ICRA)</em>, 2024
        <br>
        <a class="toggleButton">abstract</a>/
        <a href="https://arxiv.org/abs/2311.16552">paper</a>
        <br>
        <p class="myParagraph"> Various heuristic objectives for modeling hand-
          object interaction have been proposed in past work. However,
          due to the lack of a cohesive framework, these objectives
          often possess a narrow scope of applicability and are limited
          by their efficiency or accuracy. In this paper, we propose
          HANDYPRIORS, a unified and general pipeline for human-
          object interaction scenes by leveraging recent advances in
          differentiable physics and rendering. Our approach employs
          rendering priors to align with input images and segmenta-
          tion masks along with physics priors to mitigate penetration
          and relative-sliding across frames. Furthermore, we present
          two alternatives for hand and object pose estimation. The
          optimization-based pose estimation achieves higher accuracy,
          while the filtering-based tracking, which utilizes the differen-
          tiable priors as dynamics and observation models, executes
          faster. We demonstrate that HANDYPRIORS attains comparable
          or superior results in the pose estimation task, and that the
          differentiable physics module can predict contact information
          for pose refinement. We also show that our approach generalizes
          to perception tasks, including robotic hand manipulation and
          human-object pose estimation in the wild.</p>
        <p></p>
      </td>
    </tr>
				
    <tr>
      <td style="padding:20px;width:40%;display: flex;align-items: center;">
        <img src='images/icra_2023.png' width="300">
      </td>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <a href="#">
          <papertitle>Fast-Grasp’D: Dexterous Multi-finger Grasp Generation Through Differentiable Simulations</papertitle>
        </a>
        <br>
        <a href="http://www.cs.toronto.edu/~dylanturpin/">Dylan Turpin</a>,
        <a href="https://www.taozhong.info/">Tao Zhong</a>,
        <strong>Shutong Zhang</strong>,
        <a href="https://www.linkedin.com/in/guangleizhu0818/">Guanglei Zhu</a>,
        <a href="https://eric-heiden.com/">Eric Heiden</a>, 
        <a href="http://blog.mmacklin.com/">Miles Macklin</a>,
        <a href="https://tsogkas.github.io/">Stavros Tsogkas</a>,
        <a href="https://www.cs.toronto.edu/~sven/">Sven Dickinson</a>,
        <a href="https://animesh.garg.tech/">Animesh Garg</a>
        <br>
        Accepted by IEEE International Conference on Robotics and Automation <em>(ICRA)</em>, 2023
        <br>
        <a class="toggleButton">abstract</a>
        /
        <a href="https://ieeexplore.ieee.org/document/10160314">paper</a>
        /
        <a href="https://dexgrasp.github.io/">project page</a> <br>
        <p class="myParagraph"> Multi-finger grasping relies on high quality training
          data, which is hard to obtain: human data is hard to transfer
          and synthetic data relies on simplifying assumptions that reduce
          grasp quality. By making grasp simulation differentiable, and
          contact dynamics amenable to gradient-based optimization,
          we accelerate the search for high-quality grasps with fewer
          limiting assumptions. We present Grasp’D-1M: a large-scale
          dataset for multi-finger robotic grasping, synthesized with Fast-
          Grasp’D, a novel differentiable grasping simulator. Grasp’D-
          1M contains one million training examples for three robotic
          hands (three, four and five-fingered), each with multimodal
          visual inputs (RGB+depth+segmentation, available in mono and
          stereo). Grasp synthesis with Fast-Grasp’D is 10x faster than
          GraspIt! and 20x faster than the prior Grasp’D differentiable
          simulator. Generated grasps are more stable and contact-rich
          than GraspIt! grasps, regardless of the distance threshold used
          for contact generation. We validate the usefulness of our dataset
          by retraining an existing vision-based grasping pipeline
          on Grasp’D-1M, and showing a dramatic increase in model
          performance, predicting grasps with 30% more contact, a 33%
          higher epsilon metric, and 35% lower simulated displacement. </p>
        <p></p>
      </td>
    </tr>
				
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
      <tr>
        <td>
          <heading id="research-experience">Research Experience</heading>
        </td>
      </tr>
    </tbody></table>
    <table width="100%" align="center" border="0" cellpadding="20"><tbody>

          <tr>
            <td style="padding:20px;width:40%;display: flex;align-items: center;">
              <img src='images/cvl.png' width="300">
            </td>
            <td width="75%" valign="center">
              <b> ETH Zurich Computer Vision Lab </b>, Switzerland <br>
              2023.4 - present <br>
              <br>
              <b>Research Intern</b> <br>
              Supervisor: <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Prof. Luc Van Gool</a> and <a href="https://people.ee.ethz.ch/~csakarid/">Dr. Christos Sakaridis</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;display: flex;align-items: center;">
              <img src='images/pair_vector.jpg' width="300"> 
            </td>
            <td width="75%" valign="center">
              <b> PAIR Lab and Vector Institute </b>, Canada <br>
              2022.5 (project start date: 2022.8) - present <br>
              <br>
              <b>Research Intern</b> <br>
              Supervisor: <a href="https://animesh.garg.tech/" target="_blank">Prof. Animesh Garg</a>, with <a href="https://www.cs.umd.edu/~lin/" target="_blank">Prof. Ming C. Lin</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;display: flex;align-items: center;">
              <img src='images/forcolab.png' width="300" > 
            </td>
            <td width="75%" valign="center">
              <b> Forcolab </b>, Canada <br>
              2022.4 (project start date: 2022.5) - 2023.9 <br>
              <br>
              <b>Research Intern</b> <br>
              Supervisor: <a href="https://www.eecg.utoronto.ca/~shuruiz/" target="_blank">Prof. Shurui Zhou</a>, with <a href="https://jhcheng.me/" target="_blank">Prof. Jinghui Cheng</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:40%;display: flex;align-items: center;">
              <img src='images/eecg.png' width="300"> 
            </td>
            <td width="75%" valign="center">
              <b> Computer Engineering Research Group </b>, Canada <br>
              2021.5 - 2021.9 <br>
              <br>
              <b>Research Intern</b> <br>
              Supervisor: <a href="https://www.ece.utoronto.ca/people/chow-p/" target="_blank">Prof. Paul Chow</a>
            </td>
          </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <heading id="work-experience">Work Experience</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
                
                <tr>
                  <td style="padding:20px;width:40%;display: flex;align-items: center;">
                    <img src='images/intel_logo.png' width="300">
                  </td>
                  <td width="75%" valign="center">
                    <b> Intel Corporation </b>, Canada <br>
                    2022.5 - 2023.4 <br>
                    <br>
                    <b>Engineering Intern</b> <br>
                    Quality and Execution Team: Project Manager and Software Engineer <br>
                    Customer Happiness and User Experience Team: Front-End Developer <br>
                    Core Datapath Team: Complier Engineer <br>
                  </td>
                </tr>

                <tr>
                  <td style="padding:20px;width:40%;display: flex;align-items: center;">
                    <img src='images/uoft.png' width="300">
                  </td>
                  <td width="75%" valign="center">
                    <b> University of Toronto </b>, Canada <br>
                    2021.9 - 2023.4 <br>
                    <br>
                    <b>Teaching Assistant</b> <br>
                    ECE253 Digital and Computer Systems - Fall 2021, Fall 2022 <br>
                    ECE243 Computer Organization - Winter 2022, Winter 2023 <br>
                    Supervisor: Prof. Natalie Enright Jerger, Prof. Jonathan Rose
                  </td>
                </tr>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading id="teaching">Teaching and Outreach</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <p style="padding-left: 20px;"><b><a href="https://engineering.calendar.utoronto.ca/course/ece243h1">University of Toronto ECE243 Computer Organization</a></b>, Teaching Assistant. 2022 Winter, 2023 Winter</p>
          <p style="padding-left: 20px;"><b><a href="https://engineering.calendar.utoronto.ca/course/ece253h1">University of Toronto ECE253 Digital and Computer Systems</a></b>, Teaching Assistant. 2021 Fall, 2022 Fall</p>
          <p style="padding-left: 20px;"><b><a href="https://undergrad.engineering.utoronto.ca/skule-life/engineering-society/">University of Toronto Engineering</a></b>, Mentor. 2020 - present</p>
          <p style="padding-left: 20px;"><b>Rural Teaching Volunteer Program</b>, Physics Teacher. 2020</p>
          <p style="padding-left: 20px;"><b>EngFastlane</b>, Calculas and Mechanics Instructor. 2019 - 2020</p>
        
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading id="awards-and-honors">Awards and Honors</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

        <p style="padding-left: 20px;">International Experience Award ($3000). May 2023</p>
        <p style="padding-left: 20px;">University of Toronto Summer Research Exchange Fellowship ($3000). Dec 2022</p>
        <p style="padding-left: 20px;">Edith Grace Buchan Undergraduate Research Fellowship ($5400). Apr 2022</p>
        <p style="padding-left: 20px;">Department of Electrical and Computer Engineering Top Student Award. Oct 2021</p>
        <p style="padding-left: 20px;">University of Toronto In Course Scholarship ($1500). Aug 2021</p>
        <p style="padding-left: 20px;">University of Toronto Scholar. Aug 2021</p>
        <p style="padding-left: 20px;">University of Toronto Summer Research Fellowship ($5000). May 2021</p>
        <p style="padding-left: 20px;">Deans Honor List. 2019 -- 2022</p>
        <p style="padding-left: 20px;">Faculty Of Applied Science & Engineering Admission Scholarship ($5000). Sep 2019</p>

          
					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template borrowed from <a href="https://github.com/jonbarron/jonbarron_website">here</a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    
  </table>
</body>

</html>

<script>
  // JavaScript code to toggle the paragraph's visibility
    // JavaScript code to toggle the paragraphs' visibility
    document.addEventListener("DOMContentLoaded", function() {
      const toggleButtons = document.querySelectorAll(".toggleButton");

      toggleButtons.forEach((button, index) => {
        button.addEventListener("click", function() {
          const correspondingParagraph = document.querySelectorAll(".myParagraph")[index];
          
          // Check the current display style of the paragraph
          if (correspondingParagraph.style.display === "none" || correspondingParagraph.style.display === "") {
            // Show the paragraph
            correspondingParagraph.style.display = "block";
          } else {
            // Hide the paragraph
            correspondingParagraph.style.display = "none";
          }
        });
      });
    });
</script>
