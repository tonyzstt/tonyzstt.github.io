<!DOCTYPE HTML>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-67ZL1LKCB1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-67ZL1LKCB1');
  </script>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shutong Zhang</title>

  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    /* Initially, the paragraph is hidden */
    .myParagraph {
      display: none;
    }

    .toggleButton {
      cursor: pointer;
    }
  </style>

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data/favicon.png">
</head>

<body>
  <table
    style="width:100%;max-width:1100px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:70%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Shutong (Tony) Zhang</name>
                  </p>
                  <p>
                    Hello! I am a first-year MSCS student from Stanford University, specializing in Visual Computing Track. 
                    Previously, I received my Bachelor of Applied Science in Computer Engineering from the University of Toronto.
                  </p>
                  <p>
                    I'm interested in Computer Vision, Robotics, and Software Engineering. On the Vision and Robotics
                    side,
                    I wish to integrate <b>visual perceptions</b> into <b>physically plausible actions</b> to bridge the
                    gap between perception
                    and planning, and ​​enable robots to perform complex tasks with strong generalization in the 3D
                    world. On the Software
                    Engineering side, I wish to contribute to <b>collaboration beyond individuals</b>. 
                  </p>
                  <p style="text-align:center">
                    <!-- <a href="data/CV.pdf">CV</a> &nbsp/&nbsp -->
                    <a href="mailto:shutong.zhang@mail.utoronto.ca">Email</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/shutong-tony-zhang/">Linkedin</a> &nbsp/&nbsp
                    <a href="assets/CV_Shutong_Zhang.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=JYMjWq8AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                    <a href="../misc/index.html">Misc</a>
                  </p>
                  <!-- <p style="text-align:center">
                    Navigate to:
                    <a href="#education"> Education</a> /
                    <a href="#publication-and-papers"> Publications</a> /
                    <a href="#research-experience"> Research Experience</a> /
                    <a href="#work-experience"> Work Experience</a> /
                    <a href="#teaching"> Teaching & Outreach</a> /
                    <a href="#awards-and-honors"> Awards</a>
                  </p> -->
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <img style="width:85%;max-width:100%" alt="profile photo" src="images/image.png"
                    class="hoverZoomLink">
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading id="education">News</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tr>
              <p style="padding-left: 20px;">[2024.11] My paper SOLO got accepted by WACV 2025 as Oral Presentation!</p>
              <p style="padding-left: 20px;">[2024.9] My first-author paper on SE&UX Collaboration got accepted by CSCW 2025!</p>
              <p style="padding-left: 20px;">[2024.4] I will join Stanford University as a Master's student in Computer Science!</p>
              <p style="padding-left: 20px;">[2024.1] My first-author paper HandyPriors got accepted by ICRA 2024!</p>
        </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading id="education">Education</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>

              <tr>
                <td align="center" style="padding:20px;width:30%;vertical-align:middle">
                  <img src="images/stanford.webp" width="200">
                </td>
                <td width="75%" valign="middle">
                  Stanford University, CA, United States <br>
                  Master of Science in Computer Science <br>
                  2024 - 2026<br>
                  <!-- Specialized in Visual Computing<br> -->
                </td>
              </tr>


              <tr>
                <td align="center" style="padding:20px;width:30%;vertical-align:middle">
                  <img src="images/uoft.png" width="200">
                </td>
                <td width="75%" valign="middle">
                  University of Toronto, ON, Canada <br>
                  Bachelor of Applied Science <br>
                  2019 - 2024<br>
                  Major in Computer Engineering, minor in Artificial Intelligence <br>
                  CGPA: 3.94/4.0
                </td>
              </tr>




              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <heading id="publication-and-papers">Publications and Papers</heading>
                    </td>
                  </tr>
                </tbody>
              </table>
              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                  <tr>
                    <td style="padding:20px;width:40%;display: flex;align-items: center;">
                      <img src='images/solo.png' width="300">
                    </td>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <papertitle>Sun Off, Lights On: Photorealistic Monocular Nighttime Simulation for Robust Semantic Perception</papertitle>
                      <br>
                      <a href="https://icu.ee.ethz.ch/people/person-detail.MzIxMjU4.TGlzdC8zMjQxLC0xMjMyNzYwMzQy.html">Konstantinos Tzevelekakis</a>,
                      <strong>Shutong Zhang</strong>,
                      <a href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Luc Van Gool</a>,
                      <a href="https://people.ee.ethz.ch/~csakarid/">Christos Sakaridis</a>
                      <br>
                      Accepted by the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025 - <b style="color: red;">Oral Presentation</b>
                      <br>
                      <a class="toggleButton">abstract</a>
                      /
                      <a href="http://arxiv.org/abs/2407.20336">paper</a>
                      <p class="myParagraph">Nighttime scenes are hard to semantically perceive with 
                        learned models and annotate for humans. Thus, realistic synthetic nighttime 
                        data become all the more important for learning robust semantic perception at
                        night, thanks to their accurate and cheap semantic annotations. However, 
                        existing data-driven or hand-crafted techniques for generating nighttime images
                         from daytime counterparts suffer from poor realism. The reason is the complex 
                         interaction of highly spatially varying nighttime illumination, which differs 
                         drastically from its daytime counterpart, with objects of spatially varying 
                         materials in the scene, happening in 3D and being very hard to capture with 
                         such 2D approaches. The above 3D interaction and illumination shift have 
                         proven equally hard to \emph{model} in the literature, as opposed to other 
                         conditions such as fog or rain. Our method, named Sun Off, Lights On (SOLO), 
                         is the first to perform nighttime simulation on single images in a photorealistic 
                         fashion by operating in 3D. It first explicitly estimates the 3D geometry, the 
                         materials and the locations of light sources of the scene from the input daytime 
                         image and relights the scene by probabilistically instantiating light sources in 
                         a way that accounts for their semantics and then running standard ray tracing. 
                         Not only is the visual quality and photorealism of our nighttime images superior 
                         to competing approaches including diffusion models, but the former images are also 
                         proven more beneficial for semantic nighttime segmentation in day-to-night adaptation.</p>
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:20px;width:40%;display: flex;align-items: center;">
                      <img src='images/fse_2024.png' width="300">
                    </td>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <papertitle>Who is to Blame: A Comprehensive Review of Challenges and
                        Opportunities in Designer-Developer Collaboration</papertitle>
                      <br>
                      <strong>Shutong Zhang</strong>,
                      <a href="https://www.linkedin.com/in/tianyu-zhang-869b621bb/?originalSubdomain=ca">Tianyu
                        Zhang</a>,
                      <a href="https://jhcheng.me/">Jinghui Cheng</a>,
                      <a href="https://www.eecg.utoronto.ca/~shuruiz/">Shurui Zhou</a>
                      <br>
                      Accepted by the ACM Conference on Computer-Supported Cooperative Work & Social Computing (CSCW), 2025
                      <br>
                      <a class="toggleButton">abstract</a>
                      /
                      <a href="https://arxiv.org/abs/2501.11748">paper</a> /
                      <a href="https://github.com/FORCOLAB-UofToronto/CSCW2025-WhoIsToBlame">project page</a>
                      <p class="myParagraph">Software development relies on effective collaboration between Software
                        Development Engineers (SDEs)
                        and User eXperience Designers (UXDs) to create software products of high quality and usability.
                        While this
                        collaboration issue has been explored over the past decades, anecdotal evidence continues to
                        indicate the
                        existence of challenges in their collaborative efforts. To understand this gap, we first
                        conducted a systematic
                        literature review of 44 papers published since 2005, uncovering three key collaboration
                        challenges and two main
                        best practices. We then analyzed designer and developer forums and discussions on open-source
                        software
                        repositories to assess how the challenges and practices manifest in the status quo. Our findings
                        have broad
                        applicability for collaboration in software development, extending beyond the partnership
                        between SDEs and
                        UXDs. The suggested best practices and interventions also act as a reference for future
                        research, assisting in
                        the development of dedicated collaboration tools for SDEs and UXDs.</p>
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:20px;width:40%;display: flex;align-items: center;">
                      <img src='images/eth.png' width="300">
                    </td>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <papertitle>NPSim: Nighttime Photorealistic Simulation From Daytime Images With Monocular
                        Inverse Rendering and Ray Tracing</papertitle>
                      <br>
                      <strong>Shutong Zhang</strong>
                      <br>
                      Thesis at ETH Zurich Computer Vision Lab
                      <br>
                      <a class="toggleButton">abstract</a>
                      /
                      <a href="data/Thesis.pdf">paper</a>
                      /
                      <a href="data/Slides.pdf">slides</a> <br>
                      <p class="myParagraph">Semantic segmentation is an important task for autonomous driving. A
                        powerful autonomous driving system
                        should be capable of handling images under all conditions, including nighttime. Generating
                        accurate and diverse nighttime
                        semantic segmentation datasets is crucial for enhancing the performance of computer vision
                        algorithms in low-light conditions.
                        In this thesis, we introduce a novel approach named NPSim, which enables the simulation of
                        realistic nighttime images from real
                        daytime counterparts with monocular inverse rendering and ray tracing. NPSim comprises two key
                        components: mesh reconstruction
                        and relighting. The mesh reconstruction component generates an accurate representation of the
                        scene’s structure by combining
                        geometric information extracted from the input RGB image and semantic information from its
                        corresponding semantic labels. The
                        relighting component integrates real-world nighttime light sources and material characteristics
                        to simulate the complex interplay
                        of light and object surfaces under low-light conditions. The scope of this thesis mainly focuses
                        on the implementation and
                        evaluation of the mesh reconstruction component. Through experiments, we demonstrate the
                        effectiveness of the mesh reconstruction
                        component in producing high-quality scene meshes and their generality across different
                        autonomous driving datasets. We also
                        propose a detailed experiment plan for evaluating the entire pipeline, including both
                        quantitative metrics in training
                        state-of-the-art supervised and unsupervised semantic segmentation approaches and human
                        perceptual studies, aiming to indicate the
                        capability of our approach to generate realistic nighttime images and the value of our dataset
                        in steering future progress in the
                        field. NPSim not only has the ability to address the scarcity of nighttime datasets for semantic
                        segmentation, but it also has the
                        potential to improve the robustness and performance of vision algorithms under low-lighting
                        conditions.</p>
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:20px;width:40%;display: flex;align-items: center;">
                      <img src='images/icra_2024.jpg' width="300">
                    </td>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <papertitle>HandyPriors: Physically Consistent Perception of Hand-Object Interactions with
                        Differentiable Priors</papertitle>
                      <br>
                      <strong>Shutong Zhang</strong>*,
                      <a href="https://ylqiao.net/">Yiling Qiao*</a>,
                      <a href="https://www.linkedin.com/in/guangleizhu0818/">Guanglei Zhu*</a>,
                      <a href="https://eric-heiden.com/">Eric Heiden</a>,
                      <a href="http://www.cs.toronto.edu/~dylanturpin/">Dylan Turpin</a>,
                      <a href="#">Jingzhou Liu</a>,
                      <a href="https://www.cs.umd.edu/~lin/">Ming Lin</a>,
                      <a href="http://blog.mmacklin.com/">Miles Macklin</a>, <br>
                      <a href="https://animesh.garg.tech/">Animesh Garg</a>
                      <br>
                      Accepted by Conference on Computer Vision and Pattern Recognition Workshop <em>(CVPRW)</em>,
                      2023<br>
                      Accepted by IEEE International Conference on Robotics and Automation <em>(ICRA)</em>, 2024
                      <br>
                      <a class="toggleButton">abstract</a> /
                      <a href="https://arxiv.org/abs/2311.16552">paper</a> /
                      <a href="https://handypriors.github.io/">project page</a>
                      <br>
                      <p class="myParagraph"> Various heuristic objectives for modeling hand-
                        object interaction have been proposed in past work. However,
                        due to the lack of a cohesive framework, these objectives
                        often possess a narrow scope of applicability and are limited
                        by their efficiency or accuracy. In this paper, we propose
                        HANDYPRIORS, a unified and general pipeline for human-
                        object interaction scenes by leveraging recent advances in
                        differentiable physics and rendering. Our approach employs
                        rendering priors to align with input images and segmenta-
                        tion masks along with physics priors to mitigate penetration
                        and relative-sliding across frames. Furthermore, we present
                        two alternatives for hand and object pose estimation. The
                        optimization-based pose estimation achieves higher accuracy,
                        while the filtering-based tracking, which utilizes the differen-
                        tiable priors as dynamics and observation models, executes
                        faster. We demonstrate that HANDYPRIORS attains comparable
                        or superior results in the pose estimation task, and that the
                        differentiable physics module can predict contact information
                        for pose refinement. We also show that our approach generalizes
                        to perception tasks, including robotic hand manipulation and
                        human-object pose estimation in the wild.</p>
                      <p></p>
                    </td>
                  </tr>

                  <tr>
                    <td style="padding:20px;width:40%;display: flex;align-items: center;">
                      <img src='images/icra_2023.png' width="300">
                    </td>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <papertitle>Fast-Grasp’D: Dexterous Multi-finger Grasp Generation Through Differentiable
                        Simulations</papertitle>
                      <br>
                      <a href="http://www.cs.toronto.edu/~dylanturpin/">Dylan Turpin</a>,
                      <a href="https://www.taozhong.info/">Tao Zhong</a>,
                      <strong>Shutong Zhang</strong>,
                      <a href="https://www.linkedin.com/in/guangleizhu0818/">Guanglei Zhu</a>,
                      <a href="https://eric-heiden.com/">Eric Heiden</a>,
                      <a href="http://blog.mmacklin.com/">Miles Macklin</a>,
                      <a href="https://tsogkas.github.io/">Stavros Tsogkas</a>,
                      <a href="https://www.cs.toronto.edu/~sven/">Sven Dickinson</a>,
                      <a href="https://animesh.garg.tech/">Animesh Garg</a>
                      <br>
                      Accepted by IEEE International Conference on Robotics and Automation <em>(ICRA)</em>, 2023
                      <br>
                      <a class="toggleButton">abstract</a>
                      /
                      <a href="https://ieeexplore.ieee.org/document/10160314">paper</a>
                      /
                      <a href="https://dexgrasp.github.io/">project page</a> <br>
                      <p class="myParagraph"> Multi-finger grasping relies on high quality training
                        data, which is hard to obtain: human data is hard to transfer
                        and synthetic data relies on simplifying assumptions that reduce
                        grasp quality. By making grasp simulation differentiable, and
                        contact dynamics amenable to gradient-based optimization,
                        we accelerate the search for high-quality grasps with fewer
                        limiting assumptions. We present Grasp’D-1M: a large-scale
                        dataset for multi-finger robotic grasping, synthesized with Fast-
                        Grasp’D, a novel differentiable grasping simulator. Grasp’D-
                        1M contains one million training examples for three robotic
                        hands (three, four and five-fingered), each with multimodal
                        visual inputs (RGB+depth+segmentation, available in mono and
                        stereo). Grasp synthesis with Fast-Grasp’D is 10x faster than
                        GraspIt! and 20x faster than the prior Grasp’D differentiable
                        simulator. Generated grasps are more stable and contact-rich
                        than GraspIt! grasps, regardless of the distance threshold used
                        for contact generation. We validate the usefulness of our dataset
                        by retraining an existing vision-based grasping pipeline
                        on Grasp’D-1M, and showing a dramatic increase in model
                        performance, predicting grasps with 30% more contact, a 33%
                        higher epsilon metric, and 35% lower simulated displacement. </p>
                      <p></p>
                    </td>
                  </tr>

                  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                      <tr>
                        <td>
                          <heading id="research-experience">Research Experience</heading>
                        </td>
                      </tr>
                    </tbody>
                  </table>
                  <table width="100%" align="center" border="0" cellpadding="20">
                    <tbody>

                      <tr>
                        <td style="padding:20px;width:40%;display: flex;align-items: center;">
                          <img src='images/cvl.png' width="300">
                        </td>
                        <td width="75%" valign="center">
                          <b> ETH Zurich Computer Vision Lab </b>, Switzerland <br>
                          2023.4 - present <br>
                          <br>
                          <b>Research Intern</b> <br>
                          Supervisor: <a
                            href="https://ee.ethz.ch/the-department/faculty/professors/person-detail.OTAyMzM=.TGlzdC80MTEsMTA1ODA0MjU5.html">Prof.
                            Luc Van Gool</a> and <a href="https://people.ee.ethz.ch/~csakarid/">Dr. Christos
                            Sakaridis</a>
                        </td>
                      </tr>

                      <tr>
                        <td style="padding:20px;width:40%;display: flex;align-items: center;">
                          <img src='images/pair_vector.jpg' width="300">
                        </td>
                        <td width="75%" valign="center">
                          <b> PAIR Lab and Vector Institute </b>, Canada <br>
                          2022.5 (project start date: 2022.8) - present <br>
                          <br>
                          <b>Research Intern</b> <br>
                          Supervisor: <a href="https://animesh.garg.tech/" target="_blank">Prof. Animesh Garg</a>, with
                          <a href="https://www.cs.umd.edu/~lin/" target="_blank">Prof. Ming C. Lin</a>
                        </td>
                      </tr>

                      <tr>
                        <td style="padding:20px;width:40%;display: flex;align-items: center;">
                          <img src='images/forcolab.png' width="300">
                        </td>
                        <td width="75%" valign="center">
                          <b> Forcolab </b>, Canada <br>
                          2022.4 (project start date: 2022.5) - 2023.9 <br>
                          <br>
                          <b>Research Intern</b> <br>
                          Supervisor: <a href="https://www.eecg.utoronto.ca/~shuruiz/" target="_blank">Prof. Shurui
                            Zhou</a>, with <a href="https://jhcheng.me/" target="_blank">Prof. Jinghui Cheng</a>
                        </td>
                      </tr>

                      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                          <tr>
                            <td>
                              <heading id="work-experience">Work Experience</heading>
                            </td>
                          </tr>
                        </tbody>
                      </table>
                      <table width="100%" align="center" border="0" cellpadding="20">
                        <tbody>

                          <tr>
                            <td style="padding:20px;width:40%;display: flex;align-items: center;">
                              <img src='images/intel_logo.png' width="300">
                            </td>
                            <td width="75%" valign="center">
                              <b> Intel Corporation </b>, Canada <br>
                              2022.5 - 2023.4 <br>
                              <br>
                              <b>Engineering Intern</b> <br>
                              Quality and Execution Team: Project Manager and Software Engineer <br>
                              Customer Happiness and User Experience Team: Front-End Developer <br>
                              Core Datapath Team: Compiler Engineer <br>
                            </td>
                          </tr>

                          <tr>
                            <td style="padding:20px;width:40%;display: flex;align-items: center;">
                              <img src='images/uoft.png' width="300">
                            </td>
                            <td width="75%" valign="center">
                              <b> University of Toronto </b>, Canada <br>
                              2021.9 - 2023.4 <br>
                              <br>
                              <b>Teaching Assistant</b> <br>
                              ECE253 Digital and Computer Systems - Fall 2021, Fall 2022 <br>
                              ECE243 Computer Organization - Winter 2022, Winter 2023 <br>
                              Supervisor: Prof. Natalie Enright Jerger, Prof. Jonathan Rose
                            </td>
                          </tr>

                          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                            <tbody>
                              <tr>
                                <td>
                                  <heading id="teaching">Teaching and Outreach</heading>
                                </td>
                              </tr>
                            </tbody>
                          </table>
                          <table width="100%" align="center" border="0" cellpadding="20">
                            <tbody>
                              <p style="padding-left: 20px;"><b><a
                                    href="https://engineering.calendar.utoronto.ca/course/ece243h1">University of
                                    Toronto ECE243 Computer Organization</a></b>, Teaching Assistant. 2022 Winter, 2023
                                Winter</p>
                              <p style="padding-left: 20px;"><b><a
                                    href="https://engineering.calendar.utoronto.ca/course/ece253h1">University of
                                    Toronto ECE253 Digital and Computer Systems</a></b>, Teaching Assistant. 2021 Fall,
                                2022 Fall</p>
                              <p style="padding-left: 20px;"><b><a
                                    href="https://undergrad.engineering.utoronto.ca/skule-life/engineering-society/">University
                                    of Toronto Engineering</a></b>, Mentor. 2020 - present</p>
                              <p style="padding-left: 20px;"><b>Rural Teaching Volunteer Program</b>, Physics Teacher.
                                2020</p>
                              <p style="padding-left: 20px;"><b>EngFastlane</b>, Calculas and Mechanics Instructor. 2019
                                - 2020</p>

                            </tbody>
                          </table>

                          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                            <tbody>
                              <tr>
                                <td>
                                  <heading id="awards-and-honors">Awards and Honors</heading>
                                </td>
                              </tr>
                            </tbody>
                          </table>
                          <table width="100%" align="center" border="0" cellpadding="20">
                            <tbody>

                              <p style="padding-left: 20px;">International Experience Award ($3000). May 2023</p>
                              <p style="padding-left: 20px;">University of Toronto Summer Research Exchange Fellowship
                                ($3000). Dec 2022</p>
                              <p style="padding-left: 20px;">Edith Grace Buchan Undergraduate Research Fellowship
                                ($5400). Apr 2022</p>
                              <p style="padding-left: 20px;">Department of Electrical and Computer Engineering Top
                                Student Award. Oct 2021</p>
                              <p style="padding-left: 20px;">University of Toronto In Course Scholarship ($1500). Aug
                                2021</p>
                              <p style="padding-left: 20px;">University of Toronto Scholar. Aug 2021</p>
                              <p style="padding-left: 20px;">University of Toronto Summer Research Fellowship ($5000).
                                May 2021</p>
                              <p style="padding-left: 20px;">Deans Honor List. 2019 -- 2022</p>
                              <p style="padding-left: 20px;">Faculty Of Applied Science & Engineering Admission
                                Scholarship ($5000). Sep 2019</p>




                            </tbody>
                          </table>
                          <table
                            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                            <tbody>
                              <tr>
                                <td style="padding:0px">
                                  <br>
                                  <p style="text-align:right;font-size:small;">
                                    Template borrowed from <a
                                      href="https://github.com/jonbarron/jonbarron_website">here</a>.
                                  </p>
                                </td>
                              </tr>
                            </tbody>
                          </table>
        </td>

  </table>
</body>

</html>

<script>
  // JavaScript code to toggle the paragraph's visibility
  // JavaScript code to toggle the paragraphs' visibility
  document.addEventListener("DOMContentLoaded", function () {
    const toggleButtons = document.querySelectorAll(".toggleButton");

    toggleButtons.forEach((button, index) => {
      button.addEventListener("click", function () {
        const correspondingParagraph = document.querySelectorAll(".myParagraph")[index];

        // Check the current display style of the paragraph
        if (correspondingParagraph.style.display === "none" || correspondingParagraph.style.display === "") {
          // Show the paragraph
          correspondingParagraph.style.display = "block";
        } else {
          // Hide the paragraph
          correspondingParagraph.style.display = "none";
        }
      });
    });
  });
</script>